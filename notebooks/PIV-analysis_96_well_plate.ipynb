{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Particle Image Velocimetry (PIV) Analysis Workflow for xCELLigence timelapse image data**\n",
        "\n",
        "Notebook by Joanna Pylvänäinen\n",
        "\n",
        "This notebook provides a workflow for Particle Image Velocimetry (PIV) analysis for xCELLigence timelapse data. It enables the processing of multi-frame TIFF videos to compute and analyze flow metrics.\n",
        "\n",
        "Expected input data:\n",
        "- Time-lapse tiff\n",
        "- RGB images (will be converted to grayscale)\n",
        "\n",
        "Workflow outputs:\n",
        "\n",
        "- Flow field visualizations: Color-coded flow vectors between each frame.\n",
        "- Summary flow field visualizations for user defined frame interval.\n",
        "- Summary flow field visualizations for the whole video.\n",
        "- CSV metrics: Average and max velocities, average flow direction and divergence.\n",
        "- Individual plots for each video\n",
        "- Comparison plots: User defined plots for selected video combinations.\n",
        "- Averaged comparison plots: User defined plots for selected condition combinations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6NT7JtpEIPgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze6kQHhfHbvt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install and load dependencies\n",
        "\n",
        "# Install required libraries\n",
        "!pip install opencv-python numpy matplotlib openpiv tifffile natsort\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tifffile import imread, imwrite\n",
        "from scipy.ndimage import map_coordinates\n",
        "from openpiv.pyprocess import extended_search_area_piv\n",
        "from openpiv.validation import sig2noise_val\n",
        "from openpiv.filters import replace_outliers\n",
        "from openpiv.scaling import uniform\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "import gc\n",
        "import glob\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "import re\n",
        "from natsort import natsorted\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYVLj-C2ocVM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set input and output paths\n",
        "#@markdown ###Expected file format: timelapse .tif, RGB image\n",
        "\n",
        "\n",
        "input_folder = ''  # @param {type: \"string\"}\n",
        "output_folder = ''  # @param {type: \"string\"}\n",
        "\n",
        "# Count the number of files\n",
        "file_count = len([f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))])\n",
        "\n",
        "print(f\"Number of files in the folder: {file_count}\")\n",
        "\n",
        "#input_folder = '/content/drive/Shareddrives/PIV-analysis/Jammindata2_for_analysis_A-B/'  # Input folder\n",
        "#output_folder = '/content/drive/Shareddrives/PIV-analysis/Jammindata2_results_A-B/'  # Output folder\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n_k82kcNZWFf"
      },
      "outputs": [],
      "source": [
        "# @title PIV analysis (works)\n",
        "#@markdown ###Calibrate your data\n",
        "pixel_size_um = 1.73  # @param {type:\"number\"}\n",
        "time_interval_min = 20  # @param {type:\"number\"}\n",
        "#@markdown ###Define parameters for the PIV analysis\n",
        "window_size = 32 # @param {type:\"number\"}\n",
        "overlap = 16 # @param {type:\"number\"}\n",
        "search_area_size = 32 # @param {type:\"number\"}\n",
        "summary_interval = 10  # @param {type:\"number\"}\n",
        "\n",
        "# Define functions\n",
        "def load_tiff_video(file_path):\n",
        "    \"\"\"\n",
        "    Load a multi-frame TIFF video as a list of frames.\n",
        "    \"\"\"\n",
        "    print(f\"Loading: {file_path}\")\n",
        "    tiff_frames = imread(file_path)\n",
        "    if len(tiff_frames.shape) == 4:  # Multi-frame RGB TIFF\n",
        "        return [frame for frame in tiff_frames]\n",
        "    elif len(tiff_frames.shape) == 3:  # Multi-frame grayscale\n",
        "        return [tiff_frames[i] for i in range(tiff_frames.shape[0])]\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected TIFF format with shape {tiff_frames.shape}\")\n",
        "\n",
        "def convert_to_grayscale(image):\n",
        "    if len(image.shape) == 3:  # RGB image\n",
        "        return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    elif len(image.shape) == 2:  # Already grayscale\n",
        "        return image\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected image format with shape {image.shape}\")\n",
        "\n",
        "def piv_analysis(image1, image2):\n",
        "    assert len(image1.shape) == 2 and len(image2.shape) == 2, \"Images must be 2D arrays.\"\n",
        "    #window_size = 32\n",
        "    #overlap = 16\n",
        "    #search_area_size = 32\n",
        "\n",
        "    u, v, sig2noise = extended_search_area_piv(\n",
        "        image1.astype(np.int32),\n",
        "        image2.astype(np.int32),\n",
        "        window_size=window_size,\n",
        "        overlap=overlap,\n",
        "        dt=1,\n",
        "        search_area_size=search_area_size,\n",
        "    )\n",
        "    flags = sig2noise > 1.3\n",
        "    u, v = replace_outliers(u, v, flags, method='localmean', max_iter=3, kernel_size=2)\n",
        "    x, y = np.meshgrid(\n",
        "        np.arange(0, u.shape[1]) * window_size + window_size / 2,\n",
        "        np.arange(0, u.shape[0]) * window_size + window_size / 2\n",
        "    )\n",
        "    return x, y, u, v\n",
        "\n",
        "def visualize_colored_flow_field(x, y, u, v, image_shape, save_path=None):\n",
        "    angles = np.arctan2(v, u)\n",
        "    norm = plt.Normalize(vmin=-np.pi, vmax=np.pi)\n",
        "    colors = cm.hsv(norm(angles))\n",
        "    fig, ax = plt.subplots(figsize=(image_shape[1] / 100, image_shape[0] / 100), dpi=100)\n",
        "    ax.set_xlim(0, image_shape[1])\n",
        "    ax.set_ylim(image_shape[0], 0)\n",
        "    ax.axis('off')\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            ax.arrow(\n",
        "                x[i, j], y[i, j], u[i, j], v[i, j],\n",
        "                color=colors[i, j], head_width=3, head_length=3\n",
        "            )\n",
        "    if save_path:\n",
        "        fig.canvas.draw()\n",
        "        image_array = np.frombuffer(fig.canvas.buffer_rgba(), dtype='uint8')\n",
        "        image_array = image_array.reshape(int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), 4)\n",
        "        imwrite(save_path, image_array[..., :3])\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "# Updated loop for sequential file handling\n",
        "for filename in sorted(os.listdir(input_folder)):\n",
        "    if filename.endswith('.tiff') or filename.endswith('.tif'):\n",
        "        video_name = os.path.splitext(filename)[0]\n",
        "        video_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        print(f\"Processing video: {video_name}\")\n",
        "\n",
        "        # Load and preprocess the video\n",
        "        frames = load_tiff_video(video_path)\n",
        "        grayscale_frames = [convert_to_grayscale(frame) for frame in frames]\n",
        "        video_output_folder = os.path.join(output_folder, video_name)\n",
        "        os.makedirs(video_output_folder, exist_ok=True)\n",
        "\n",
        "        # Subfolders for flow field images and summary intervals\n",
        "        frame_pairs_folder = os.path.join(video_output_folder, \"frame_pairs\")\n",
        "        os.makedirs(frame_pairs_folder, exist_ok=True)\n",
        "        summary_intervals_folder = os.path.join(video_output_folder, \"summary_intervals\")\n",
        "        os.makedirs(summary_intervals_folder, exist_ok=True)\n",
        "\n",
        "        # Initialize metrics and cumulative flow variables\n",
        "        metrics_list = []\n",
        "        cumulative_u, cumulative_v = None, None\n",
        "        frame_counter = 0\n",
        "        overall_u, overall_v = None, None\n",
        "\n",
        "        for i in range(len(grayscale_frames) - 1):\n",
        "            img1 = grayscale_frames[i]\n",
        "            img2 = grayscale_frames[i + 1]\n",
        "            x, y, u_raw, v_raw = piv_analysis(img1, img2)\n",
        "\n",
        "            # Compute calibrated values for metrics\n",
        "            u_calibrated = u_raw * pixel_size_um / time_interval_min\n",
        "            v_calibrated = v_raw * pixel_size_um / time_interval_min\n",
        "            flow_magnitudes_calibrated = np.sqrt(u_calibrated**2 + v_calibrated**2)\n",
        "\n",
        "            # Compute metrics\n",
        "            avg_velocity = flow_magnitudes_calibrated.mean()\n",
        "            avg_direction = np.arctan2(v_calibrated, u_calibrated).mean()\n",
        "            max_magnitude = flow_magnitudes_calibrated.max()\n",
        "            divergence = np.gradient(u_calibrated, axis=1).mean() + np.gradient(v_calibrated, axis=0).mean()\n",
        "            end_time_point = (i + 1) * time_interval_min\n",
        "\n",
        "            metrics_list.append({\n",
        "                \"Time Point\": f\"{i+1}-{i+2}\",\n",
        "                \"End Time Point (min)\": end_time_point,\n",
        "                \"Avg Velocity (µm/min)\": avg_velocity,\n",
        "                \"Avg Direction (degrees)\": np.degrees(avg_direction),\n",
        "                \"Max Flow Magnitude (µm/min)\": max_magnitude,\n",
        "                \"Divergence (1/min)\": divergence\n",
        "            })\n",
        "\n",
        "            # Update cumulative raw flows for visualization\n",
        "            if overall_u is None:\n",
        "                overall_u, overall_v = u_raw, v_raw\n",
        "            else:\n",
        "                overall_u += u_raw\n",
        "                overall_v += v_raw\n",
        "\n",
        "            if cumulative_u is None:\n",
        "                cumulative_u, cumulative_v = u_raw, v_raw\n",
        "            else:\n",
        "                cumulative_u += u_raw\n",
        "                cumulative_v += v_raw\n",
        "\n",
        "            # Save individual flow field visualization\n",
        "            flow_field_path = os.path.join(frame_pairs_folder, f\"flow_{i+1}-{i+2}.tiff\")\n",
        "            visualize_colored_flow_field(x, y, u_raw, v_raw, img1.shape, save_path=flow_field_path)\n",
        "\n",
        "            # Clear memory for individual flow fields\n",
        "            del u_raw, v_raw\n",
        "            gc.collect()\n",
        "\n",
        "            # Save cumulative flow at specified interval\n",
        "            frame_counter += 1\n",
        "            if frame_counter == summary_interval or i == len(grayscale_frames) - 2:\n",
        "                summary_path = os.path.join(summary_intervals_folder, f\"summary_flow_{i+1}.tiff\")\n",
        "                visualize_colored_flow_field(x, y, cumulative_u, cumulative_v, img1.shape, save_path=summary_path)\n",
        "                cumulative_u, cumulative_v = None, None  # Reset cumulative flow\n",
        "                frame_counter = 0\n",
        "                gc.collect()\n",
        "\n",
        "        # Save metrics to CSV\n",
        "        metrics_df = pd.DataFrame(metrics_list)\n",
        "        summary_csv_path = os.path.join(video_output_folder, f\"{video_name}_flow_summary.csv\")\n",
        "        metrics_df.to_csv(summary_csv_path, index=False)\n",
        "        print(f\"Metrics saved to: {summary_csv_path}\")\n",
        "\n",
        "        # Save overall summary flow field\n",
        "        summary_image_path = os.path.join(video_output_folder, \"summary_flow_field.tiff\")\n",
        "        visualize_colored_flow_field(x, y, overall_u, overall_v, grayscale_frames[0].shape, save_path=summary_image_path)\n",
        "        print(f\"Summary flow field saved to: {summary_image_path}\")\n",
        "\n",
        "        # Clear memory for the next file\n",
        "        del frames, grayscale_frames, cumulative_u, cumulative_v, metrics_df, overall_u, overall_v\n",
        "        gc.collect()\n",
        "        print(f\"Memory cleared for: {video_name}\")\n",
        "\n",
        "def concatenate_csv_files(input_folder, output_folder, output_filename=\"combined_results.csv\"):\n",
        "    \"\"\"\n",
        "    Concatenate all CSV files in the input folder and its subfolders into a single CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - input_folder: Path to the folder containing individual CSV files in subfolders.\n",
        "    - output_folder: Path to the folder where the combined CSV file will be saved.\n",
        "    - output_filename: Name of the output CSV file.\n",
        "    \"\"\"\n",
        "    # Find all CSV files in the input folder and its subfolders\n",
        "    csv_files = glob.glob(os.path.join(output_folder, \"**\", \"*_flow_summary.csv\"), recursive=True)\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"No CSV files found in the output_folder: {output_folder}\")\n",
        "        return\n",
        "\n",
        "    # Debug: List found files\n",
        "    print(f\"Found {len(csv_files)} CSV files:\")\n",
        "    for file in csv_files:\n",
        "        print(f\" - {file}\")\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            # Extract the CSV file name (without the folder path)\n",
        "            csv_file_name = os.path.basename(csv_file)\n",
        "\n",
        "            # Read the CSV file\n",
        "            metrics_df = pd.read_csv(csv_file)\n",
        "\n",
        "            # Add a column to track the source file name\n",
        "            metrics_df[\"CSV File Name\"] = csv_file_name\n",
        "\n",
        "            # Concatenate the data\n",
        "            combined_df = pd.concat([combined_df, metrics_df], ignore_index=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {csv_file}: {e}\")\n",
        "\n",
        "    # Save the combined DataFrame to a single CSV file\n",
        "    output_path = os.path.join(output_folder, output_filename)\n",
        "    combined_df.to_csv(output_path, index=False)\n",
        "    print(f\"Combined results saved to: {output_path}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "\n",
        "concatenate_csv_files(input_folder, output_folder, output_filename=\"combined_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select graphs to plot for each video\n",
        "plot_avg_velocity = True  # @param {type: \"boolean\"}\n",
        "plot_avg_direction = True  # @param {type: \"boolean\"}\n",
        "plot_max_flow_magnitude = True  # @param {type: \"boolean\"}\n",
        "plot_divergence = True  # @param {type: \"boolean\"}\n",
        "\n",
        "def plot_metrics_from_csv(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Generate improved plots for each video based on saved CSV files.\n",
        "\n",
        "    Parameters:\n",
        "    - input_folder: Folder containing the CSV files.\n",
        "    - output_folder: Folder to save plots.\n",
        "    - plot_avg_velocity: Boolean to plot Average Velocity.\n",
        "    - plot_avg_direction: Boolean to plot Average Direction.\n",
        "    - plot_max_flow_magnitude: Boolean to plot Maximum Flow Magnitude.\n",
        "    - plot_divergence: Boolean to plot Divergence.\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(output_folder, \"**\", \"*_flow_summary.csv\"), recursive=True)\n",
        "\n",
        "    if not csv_files:\n",
        "      print(\"No CSV files found.\")\n",
        "      return\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        video_name = os.path.basename(csv_file).replace(\"_flow_summary.csv\", \"\")\n",
        "        metrics_df = pd.read_csv(csv_file)\n",
        "\n",
        "        video_output_folder = os.path.join(output_folder, video_name)\n",
        "        os.makedirs(video_output_folder, exist_ok=True)\n",
        "\n",
        "        # Use \"End Time Point (min)\" as x-axis\n",
        "        x_values = metrics_df[\"End Time Point (min)\"]\n",
        "\n",
        "        if plot_avg_velocity:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(x_values, metrics_df[\"Avg Velocity (µm/min)\"], marker='o', color='#9467bd', label='Avg Velocity')\n",
        "            plt.title(f\"Average Velocity - {video_name}\", fontsize=16)\n",
        "            plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "            plt.ylabel(\"Average Velocity (µm/min)\", fontsize=14)\n",
        "            plt.xticks(fontsize=12)\n",
        "            plt.yticks(fontsize=12)\n",
        "            plt.legend(fontsize=12)\n",
        "            avg_velocity_path = os.path.join(video_output_folder, f\"{video_name}_avg_velocity.png\")\n",
        "            plt.savefig(avg_velocity_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Avg Velocity plot saved to: {avg_velocity_path}\")\n",
        "\n",
        "        if plot_avg_direction:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(x_values, metrics_df[\"Avg Direction (degrees)\"], marker='s', color='#ff7f0e', label='Avg Direction (degrees)')\n",
        "            plt.title(f\"Average Flow Direction - {video_name}\", fontsize=16)\n",
        "            plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "            plt.ylabel(\"Average Direction (degrees)\", fontsize=14)\n",
        "            plt.xticks(fontsize=12)\n",
        "            plt.yticks(fontsize=12)\n",
        "            plt.legend(fontsize=12)\n",
        "            avg_direction_path = os.path.join(video_output_folder, f\"{video_name}_avg_direction.png\")\n",
        "            plt.savefig(avg_direction_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Avg Direction plot saved to: {avg_direction_path}\")\n",
        "\n",
        "        if plot_max_flow_magnitude:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(x_values, metrics_df[\"Max Flow Magnitude (µm/min)\"], marker='^', color='#2ca02c', label='Max Flow Magnitude')\n",
        "            plt.title(f\"Maximum Flow Magnitude - {video_name}\", fontsize=16)\n",
        "            plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "            plt.ylabel(\"Max Flow Magnitude (µm/min)\", fontsize=14)\n",
        "            plt.xticks(fontsize=12)\n",
        "            plt.yticks(fontsize=12)\n",
        "            plt.legend(fontsize=12)\n",
        "            max_flow_magnitude_path = os.path.join(video_output_folder, f\"{video_name}_max_flow_magnitude.png\")\n",
        "            plt.savefig(max_flow_magnitude_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Max Flow Magnitude plot saved to: {max_flow_magnitude_path}\")\n",
        "\n",
        "        if plot_divergence:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(x_values, metrics_df[\"Divergence (1/min)\"], marker='d', color='#d62728', label='Divergence')\n",
        "            plt.title(f\"Divergence Over Time - {video_name}\", fontsize=16)\n",
        "            plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "            plt.ylabel(\"Divergence (1/min)\", fontsize=14)\n",
        "            plt.xticks(fontsize=12)\n",
        "            plt.yticks(fontsize=12)\n",
        "            plt.axhline(0, color='black', linestyle='--', linewidth=1, label='Zero Divergence')\n",
        "            plt.legend(fontsize=12)\n",
        "            divergence_path = os.path.join(video_output_folder, f\"{video_name}_divergence.png\")\n",
        "            plt.savefig(divergence_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Divergence plot saved to: {divergence_path}\")\n",
        "\n",
        "\n",
        "plot_metrics_from_csv(input_folder, output_folder)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Oorkl8zdoLia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Map conditions of your data\n",
        "\n",
        "def map_conditions_and_save(output_path):\n",
        "    \"\"\"\n",
        "    Map conditions to identifiers and save the updated compiled CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - output_path: Path to the combined CSV file containing the \"CSV File Name\" column.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the compiled CSV file\n",
        "        compiled_data = pd.read_csv(output_path)\n",
        "\n",
        "        # Extract unique identifiers from the \"CSV File Name\" column\n",
        "        compiled_data['Identifier'] = compiled_data['CSV File Name'].str.extract(r'(^[A-Za-z0-9]+)_')\n",
        "        unique_identifiers = compiled_data['Identifier'].dropna().unique()\n",
        "\n",
        "\n",
        "        # Assuming unique_identifiers is a list or iterable\n",
        "        unique_identifiers = natsorted(unique_identifiers)  # Sort identifiers naturally\n",
        "\n",
        "        # Create widgets for each identifier\n",
        "        identifier_widgets = {}\n",
        "        for identifier in unique_identifiers:\n",
        "            identifier_widgets[identifier] = widgets.Text(\n",
        "                value='',\n",
        "                placeholder='Enter condition name',\n",
        "                description=f'{identifier}:',\n",
        "                style={'description_width': 'initial'}\n",
        "            )\n",
        "\n",
        "        # Create a button to submit the mapping\n",
        "        submit_button = widgets.Button(description=\"Submit Mapping\")\n",
        "\n",
        "        # Output display for results\n",
        "        output = widgets.Output()\n",
        "\n",
        "        def on_submit_button_click(b):\n",
        "            with output:\n",
        "                clear_output()\n",
        "                print(\"Condition Mapping Results:\")\n",
        "\n",
        "                # Create a mapping from the user inputs\n",
        "                condition_mapping = {\n",
        "                    identifier: widget.value\n",
        "                    for identifier, widget in identifier_widgets.items()\n",
        "                }\n",
        "                print(condition_mapping)\n",
        "\n",
        "                # Add the conditions to the compiled data\n",
        "                compiled_data['Condition'] = compiled_data['Identifier'].map(condition_mapping)\n",
        "\n",
        "                # Save the updated data back to the same file or a new file\n",
        "                compiled_data.to_csv(output_path, index=False)\n",
        "                print(f\"Updated compiled data with conditions saved to: {output_path}\")\n",
        "\n",
        "        submit_button.on_click(on_submit_button_click)\n",
        "\n",
        "        # Display all widgets\n",
        "        display(widgets.VBox(list(identifier_widgets.values()) + [submit_button, output]))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "# Example usage from the previous cell\n",
        "output_filename = \"combined_results.csv\"\n",
        "output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "# Call the function with the saved combined CSV file\n",
        "map_conditions_and_save(output_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lAqWCKMWux9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot individual data for each identifier\n",
        "\n",
        "def plot_individual_data(output_folder, x_tick_interval=200):\n",
        "    \"\"\"\n",
        "    Plot individual data for each identifier based on user-selected conditions and metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - output_folder: Path where the concatenated CSV file is located and where plots will be saved.\n",
        "    - x_tick_interval: Interval for x-axis labels.\n",
        "    \"\"\"\n",
        "    # Locate the concatenated CSV file\n",
        "    csv_files = glob.glob(os.path.join(output_folder, \"**\", \"combined_results.csv\"), recursive=True)\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"No concatenated CSV file found in the output folder: {output_folder}\")\n",
        "        return\n",
        "\n",
        "    concatenated_csv = csv_files[0]\n",
        "    print(f\"Using concatenated file: {concatenated_csv}\")\n",
        "\n",
        "    # Read the concatenated CSV file\n",
        "    data = pd.read_csv(concatenated_csv)\n",
        "\n",
        "    # Extract unique conditions and identifiers\n",
        "    data[\"Identifier\"] = data[\"CSV File Name\"].str.extract(r'(^[A-Za-z0-9]+)_')\n",
        "    unique_conditions = data.groupby(\"Identifier\")[\"Condition\"].first().reset_index()\n",
        "\n",
        "    # Possible metrics to check in the CSV columns\n",
        "    possible_metrics = {\n",
        "        \"Avg Velocity (µm/min)\": \"Avg Velocity\",\n",
        "        \"Avg Direction (degrees)\": \"Avg Direction\",\n",
        "        \"Max Flow Magnitude (µm/min)\": \"Max Flow Magnitude\",\n",
        "        \"Divergence (1/min)\": \"Divergence\"\n",
        "    }\n",
        "\n",
        "    # Filter metrics based on existing columns in the CSV\n",
        "    available_metrics = {col: name for col, name in possible_metrics.items() if col in data.columns}\n",
        "\n",
        "    # Create widgets for the available metrics\n",
        "    metrics_checkboxes = {\n",
        "        name: widgets.Checkbox(value=False, description=name, indent=False)\n",
        "        for _, name in available_metrics.items()\n",
        "    }\n",
        "\n",
        "    # Ensure unique_conditions is sorted naturally by \"Identifier\"\n",
        "    if \"Identifier\" in unique_conditions.columns:\n",
        "        # Apply natural sorting to the DataFrame\n",
        "        unique_conditions = unique_conditions.iloc[\n",
        "            natsorted(unique_conditions.index, key=lambda i: unique_conditions.loc[i, \"Identifier\"])\n",
        "        ]\n",
        "    else:\n",
        "        print(\"The column 'Identifier' does not exist in unique_conditions.\")\n",
        "\n",
        "    # Widgets for user input\n",
        "    checkboxes = {}\n",
        "    for _, row in unique_conditions.iterrows():  # Iterating through naturally sorted DataFrame\n",
        "        identifier = row[\"Identifier\"]\n",
        "        condition = row[\"Condition\"]\n",
        "        checkboxes[identifier] = widgets.Checkbox(\n",
        "            value=False,\n",
        "            description=f\"{identifier} - {condition}\",\n",
        "            indent=False\n",
        "        )\n",
        "\n",
        "\n",
        "    smoothing_window_widget = widgets.IntText(\n",
        "        value=3,\n",
        "        description=\"Smoothing Window:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(description=\"Plot Individual Data\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def plot_selected_individual(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            selected_identifiers = [key for key, checkbox in checkboxes.items() if checkbox.value]\n",
        "            selected_metrics = [metric for metric, checkbox in metrics_checkboxes.items() if checkbox.value]\n",
        "            smoothing_window = smoothing_window_widget.value\n",
        "\n",
        "            if not selected_identifiers:\n",
        "                print(\"No conditions selected for plotting.\")\n",
        "                return\n",
        "\n",
        "            if not selected_metrics:\n",
        "                print(\"No metrics selected for plotting.\")\n",
        "                return\n",
        "\n",
        "            # Ensure the output folder exists\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "            # Create a sanitized identifier list for the filename\n",
        "            identifiers_string = \"_\".join(re.sub(r'[^\\w\\s-]', '', identifier) for identifier in selected_identifiers)\n",
        "\n",
        "            for metric in selected_metrics:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for identifier in selected_identifiers:\n",
        "                    group_data = data[data[\"Identifier\"] == identifier]\n",
        "\n",
        "                    if group_data.empty:\n",
        "                        print(f\"No data found for identifier: {identifier}\")\n",
        "                        continue\n",
        "\n",
        "                    time_labels = group_data[\"End Time Point (min)\"]\n",
        "                    metric_column = [col for col, name in available_metrics.items() if name == metric][0]\n",
        "                    metric_values = group_data[metric_column]\n",
        "\n",
        "                    smoothed_values = uniform_filter1d(metric_values, size=smoothing_window)\n",
        "\n",
        "                    # Add condition to the label\n",
        "                    condition = unique_conditions[unique_conditions[\"Identifier\"] == identifier][\"Condition\"].values[0]\n",
        "                    label = f\"{identifier} - {condition}\"\n",
        "\n",
        "                    plt.plot(\n",
        "                        time_labels,\n",
        "                        smoothed_values,\n",
        "                        marker='o',\n",
        "                        label=label,\n",
        "                        alpha=0.7\n",
        "                    )\n",
        "\n",
        "                # Dynamically include units in the y-label\n",
        "                y_label = metric_column\n",
        "\n",
        "                plt.title(f\"{metric} - Individual Data\", fontsize=16)\n",
        "                plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "                plt.ylabel(y_label, fontsize=14)\n",
        "                plt.legend(loc=\"upper right\", fontsize=12)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Sanitize the metric and identifiers for the file name\n",
        "                sanitized_metric = re.sub(r'[^\\w\\s-]', '', metric.replace(\" \", \"_\").lower())\n",
        "                plot_filename = f\"{sanitized_metric}_({identifiers_string})_individual.png\"\n",
        "                plot_path = os.path.join(output_folder, plot_filename)\n",
        "\n",
        "                plt.savefig(plot_path, bbox_inches='tight')\n",
        "                plt.show()\n",
        "                print(f\"Individual plot saved for {metric} - {', '.join(selected_identifiers)}: {plot_path}\")\n",
        "\n",
        "    submit_button.on_click(plot_selected_individual)\n",
        "\n",
        "    display(widgets.VBox(\n",
        "        list(checkboxes.values()) +\n",
        "        [widgets.Label(\"Select Metrics to Plot:\")] +\n",
        "        list(metrics_checkboxes.values()) +\n",
        "        [smoothing_window_widget, submit_button, output]\n",
        "    ))\n",
        "\n",
        "plot_individual_data(output_folder=output_folder, x_tick_interval=200)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rn7vxlf2z81E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot averaged data by condition with error bars\n",
        "\n",
        "def plot_averaged_data(output_folder, x_tick_interval=200):\n",
        "    \"\"\"\n",
        "    Plot averaged data by condition with error bars based on user-selected metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - output_folder: Path where the concatenated CSV file is located and where plots will be saved.\n",
        "    - x_tick_interval: Interval for x-axis labels.\n",
        "    \"\"\"\n",
        "    # Locate the concatenated CSV file\n",
        "    csv_files = glob.glob(os.path.join(output_folder, \"**\", \"combined_results.csv\"), recursive=True)\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"No concatenated CSV file found in the output folder: {output_folder}\")\n",
        "        return\n",
        "\n",
        "    concatenated_csv = csv_files[0]\n",
        "    print(f\"Using concatenated file: {concatenated_csv}\")\n",
        "\n",
        "    # Read the concatenated CSV file\n",
        "    data = pd.read_csv(concatenated_csv)\n",
        "\n",
        "    # Extract unique conditions and identifiers\n",
        "    data[\"Identifier\"] = data[\"CSV File Name\"].str.extract(r'(^[A-Za-z0-9]+)_')\n",
        "    unique_conditions = data[\"Condition\"].drop_duplicates()\n",
        "\n",
        "    # Apply natural sorting to unique_conditions\n",
        "    unique_conditions = natsorted(unique_conditions)\n",
        "\n",
        "    # Possible metrics to check in the CSV columns\n",
        "    possible_metrics = {\n",
        "        \"Avg Velocity (µm/min)\": \"Avg Velocity\",\n",
        "        \"Avg Direction (degrees)\": \"Avg Direction\",\n",
        "        \"Max Flow Magnitude (µm/min)\": \"Max Flow Magnitude\",\n",
        "        \"Divergence (1/min)\": \"Divergence\"\n",
        "    }\n",
        "\n",
        "    # Filter metrics based on existing columns in the CSV\n",
        "    available_metrics = {col: name for col, name in possible_metrics.items() if col in data.columns}\n",
        "\n",
        "    # Create widgets for the available metrics\n",
        "    metrics_checkboxes = {\n",
        "        name: widgets.Checkbox(value=False, description=name, indent=False)\n",
        "        for _, name in available_metrics.items()\n",
        "    }\n",
        "\n",
        "    # Widgets for user input\n",
        "    checkboxes = {}\n",
        "    for condition in unique_conditions:\n",
        "        checkboxes[condition] = widgets.Checkbox(\n",
        "            value=False,\n",
        "            description=condition,\n",
        "            indent=False\n",
        "        )\n",
        "\n",
        "    smoothing_window_widget = widgets.IntText(\n",
        "        value=3,\n",
        "        description=\"Smoothing Window:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(description=\"Plot Averaged Data\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def plot_selected_averaged(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            selected_conditions = [key for key, checkbox in checkboxes.items() if checkbox.value]\n",
        "            selected_metrics = [metric for metric, checkbox in metrics_checkboxes.items() if checkbox.value]\n",
        "            smoothing_window = smoothing_window_widget.value\n",
        "\n",
        "            if not selected_conditions:\n",
        "                print(\"No conditions selected for plotting.\")\n",
        "                return\n",
        "\n",
        "            if not selected_metrics:\n",
        "                print(\"No metrics selected for plotting.\")\n",
        "                return\n",
        "\n",
        "            # Ensure the output folder exists\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "            # Create a sanitized condition list for the filename\n",
        "            conditions_string = \"_\".join(re.sub(r'[^\\w\\s-]', '', condition) for condition in selected_conditions)\n",
        "\n",
        "            for metric in selected_metrics:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                grouped_data = data[data[\"Condition\"].isin(selected_conditions)].groupby(\"Condition\")\n",
        "\n",
        "                for condition, group in grouped_data:\n",
        "                    if group.empty:\n",
        "                        print(f\"No data found for condition: {condition}\")\n",
        "                        continue\n",
        "\n",
        "                    # Preprocess: Convert the metric column to numeric\n",
        "                    metric_column = [col for col, name in available_metrics.items() if name == metric][0]\n",
        "                    group[metric_column] = pd.to_numeric(group[metric_column], errors='coerce')\n",
        "\n",
        "                    # Drop rows with NaN values in the metric column\n",
        "                    group = group.dropna(subset=[metric_column])\n",
        "\n",
        "                    # Calculate mean and standard deviation for each time point\n",
        "                    time_points = group[\"End Time Point (min)\"].unique()\n",
        "                    mean_values = group.groupby(\"End Time Point (min)\")[metric_column].mean()\n",
        "                    std_values = group.groupby(\"End Time Point (min)\")[metric_column].std()\n",
        "\n",
        "                    # Apply smoothing\n",
        "                    smoothed_mean = uniform_filter1d(mean_values, size=smoothing_window)\n",
        "                    smoothed_std = uniform_filter1d(std_values, size=smoothing_window)\n",
        "\n",
        "                    # Plot data with error bars\n",
        "                    plt.errorbar(\n",
        "                        time_points,\n",
        "                        smoothed_mean,\n",
        "                        yerr=smoothed_std,\n",
        "                        marker='o',\n",
        "                        label=f\"{condition}\",\n",
        "                        alpha=0.7\n",
        "                    )\n",
        "\n",
        "                # Dynamically include units in the y-label\n",
        "                y_label = metric_column\n",
        "\n",
        "                plt.title(f\"{metric} - Averaged by Condition\", fontsize=16)\n",
        "                plt.xlabel(\"Time (min)\", fontsize=14)\n",
        "                plt.ylabel(y_label, fontsize=14)\n",
        "                plt.legend(loc=\"upper right\", fontsize=12)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Sanitize the metric and condition list for the file name\n",
        "                sanitized_metric = re.sub(r'[^\\w\\s-]', '', metric.replace(\" \", \"_\").lower())\n",
        "                plot_filename = f\"{sanitized_metric}_({conditions_string})_averaged.png\"\n",
        "                plot_path = os.path.join(output_folder, plot_filename)\n",
        "\n",
        "                plt.savefig(plot_path, bbox_inches='tight')\n",
        "                plt.show()\n",
        "                print(f\"Averaged plot saved for {metric} - {', '.join(selected_conditions)}: {plot_path}\")\n",
        "\n",
        "    submit_button.on_click(plot_selected_averaged)\n",
        "\n",
        "    display(widgets.VBox(\n",
        "        list(checkboxes.values()) +\n",
        "        [widgets.Label(\"Select Metrics to Plot:\")] +\n",
        "        list(metrics_checkboxes.values()) +\n",
        "        [smoothing_window_widget, submit_button, output]\n",
        "    ))\n",
        "\n",
        "plot_averaged_data(output_folder=output_folder, x_tick_interval=200)\n",
        "\n"
      ],
      "metadata": {
        "id": "E7qkVcCq-B2t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}